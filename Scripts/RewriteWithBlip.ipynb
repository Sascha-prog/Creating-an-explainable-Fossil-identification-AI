{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39f7b2a7",
   "metadata": {},
   "source": [
    "## Section 1\n",
    "\n",
    "AtA (Agent-to-Agent) notebook using BLIP for captioning + Grad-CAM explainability.\n",
    "\n",
    "Features:\n",
    "- Fine-tuned BLIP-2 (base/small) on my image->caption JSON dataset (HuggingFace Trainer).\n",
    "- Inference agents: CaptionAgent (BLIP generate) and ExplainAgent (cross-attention heatmap).\n",
    "- Grad-CAM that hooks BLIP cross-attention and produces a 2D heatmap overlay.\n",
    "\n",
    "Notes:\n",
    "- This file assumes dataset JSON mapping: {\"img.jpg\": \"caption text\", ...} located at json_path.\n",
    "\n",
    "Importing all the librariers we need and Parameter initialisation:\n",
    "- Creating the path for the dataset.\n",
    "- Selecting the correct BLIP model (have it run on GPU).\n",
    "- Loading the OpenClip Model from OpenAI with  its processor.\n",
    "- Load my custom weights into the model.\n",
    "- Create training parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aebdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, TrainingArguments, Trainer, CLIPProcessor, CLIPModel\n",
    "from datasets import Dataset as HFDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cded8215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Config ===\n",
    "json_path = \"dataset/dataset.json\"         \n",
    "image_folder = \"dataset/images/\"        \n",
    "model_name = \"Salesforce/blip-image-captioning-base\"  # change to -small if memory limited\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load CLIP\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "state = torch.load(\"model/finetuned_clip.pt\", map_location=device)\n",
    "clip_model.load_state_dict(state, strict=False)\n",
    "clip_model = clip_model.to(\"cuda\").eval()\n",
    "\n",
    "\n",
    "# Training hyperparams\n",
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 2\n",
    "num_train_epochs = 5\n",
    "learning_rate = 1e-5\n",
    "output_dir = \"blip_finetuned\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a754e97f",
   "metadata": {},
   "source": [
    "## Section 2\n",
    "Utilities.\n",
    "In order to make the usage as easy as possible a number of utility functions are implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e84ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Utilities ===\n",
    "class SimpleCaptionDataset(Dataset):\n",
    "    def __init__(self, json_file, images_dir, processor):\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        # data expected as {\"img.jpg\": \"caption\"}\n",
    "        self.items = list(data.items())\n",
    "        self.images_dir = images_dir\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name, caption = self.items[idx]\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        # processor will be used in collate\n",
    "        return {\"image\": image, \"caption\": caption, \"img_name\": img_name}\n",
    "    \n",
    "        \n",
    "def clip_caption_confidence(image_pil, caption):\n",
    "    \"\"\"\n",
    "    Returns (clip_caption, confidence)\n",
    "    where clip_caption = caption you fed in\n",
    "    and confidence = softmax probability of this caption among itself.\n",
    "    \"\"\"\n",
    "    # Package image + caption for CLIP\n",
    "    inputs = clip_processor(\n",
    "        text=[caption],\n",
    "        images=image_pil,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model(**inputs)\n",
    "        logits = outputs.logits_per_image  # [1, 1]\n",
    "        probs = logits.softmax(dim=-1)\n",
    "    \n",
    "    confidence = probs[0, 0].item()\n",
    "    return caption, confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8108c6d",
   "metadata": {},
   "source": [
    "## Section 3\n",
    "Agent Framework Setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537b739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple caption agent\n",
    "def make_caption_agent(processor, model):\n",
    "    def caption_agent(message):\n",
    "        image = message[\"image\"]\n",
    "        inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            out_ids = model.generate(**inputs, max_new_tokens=40)\n",
    "        caption = processor.decode(out_ids[0], skip_special_tokens=True)\n",
    "        message[\"caption\"] = caption\n",
    "        return message\n",
    "    return caption_agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5894b8",
   "metadata": {},
   "source": [
    "## Section 4\n",
    "Loading Blip and creating a trainer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd91dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate for trainer (returns pixel_values and labels)\n",
    "def collate_fn(batch, processor):\n",
    "    images = [b[\"image\"] for b in batch]\n",
    "    captions = [b[\"caption\"] for b in batch]\n",
    "    enc = processor(images=images, text=captions, return_tensors=\"pt\", padding=True)\n",
    "    # enc contains pixel_values and input_ids (decoder) depending on processor\n",
    "    # Ensure labels are set: replace pad token id with -100\n",
    "    labels = enc[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    return {\n",
    "        \"pixel_values\": enc[\"pixel_values\"],\n",
    "        \"labels\": labels\n",
    "    }\n",
    "# === Training helper (HuggingFace Trainer) ===\n",
    "def prepare_hf_dataset(json_path, image_folder, processor):\n",
    "    ds = SimpleCaptionDataset(json_path, image_folder, processor)\n",
    "    # convert to HF dataset wrapper for Trainer convenience\n",
    "    records = []\n",
    "    for item in ds:\n",
    "        # store image path and caption text\n",
    "        records.append({\"image\": item[\"image\"], \"caption\": item[\"caption\"]})\n",
    "    # Note: HF dataset can store PIL images in memory; Trainer will use collate_fn provided\n",
    "    hf = HFDataset.from_list(records)\n",
    "    return hf\n",
    "\n",
    "\n",
    "def train_blip(json_path=json_path, image_folder=image_folder, model_name=model_name, out_dir=output_dir):\n",
    "    print(\"Loading processor and model:\", model_name)\n",
    "    processor = BlipProcessor.from_pretrained(model_name)\n",
    "    model = BlipForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "    # Freeze most params then unfreeze last layers if desired\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # Heuristic: unfreeze last 3 vision blocks + cross-attention + LM head\n",
    "    try:\n",
    "        vision = model.vision_model\n",
    "        # some BLIP variants: vision_model.encoder.layer\n",
    "        blocks = getattr(vision, 'encoder').layer if hasattr(getattr(vision, 'encoder', None), 'layer') else vision.transformer.resblocks\n",
    "        for blk in blocks[-3:]:\n",
    "            for p in blk.parameters():\n",
    "                p.requires_grad = True\n",
    "    except Exception:\n",
    "        # fallback: try model.vision_model.transformer.resblocks\n",
    "        try:\n",
    "            for blk in model.vision_model.transformer.resblocks[-3:]:\n",
    "                for p in blk.parameters():\n",
    "                    p.requires_grad = True\n",
    "        except Exception:\n",
    "            print(\"Warning: could not auto-unfreeze vision blocks; you may unfreeze manually.\")\n",
    "\n",
    "    # Unfreeze LM head\n",
    "    for name, p in model.named_parameters():\n",
    "        if \"lm_head\" in name or \"qformer\" in name or \"text\" in name:\n",
    "            p.requires_grad = True\n",
    "\n",
    "    processor = BlipProcessor.from_pretrained(model_name)\n",
    "    hf_ds = prepare_hf_dataset(json_path, image_folder, processor)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=100,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "    def data_collator(batch):\n",
    "        return collate_fn(batch, processor)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=hf_ds,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(out_dir)\n",
    "    processor.save_pretrained(out_dir)\n",
    "    print(\"Saved fine-tuned BLIP to\", out_dir)\n",
    "    return out_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d94519",
   "metadata": {},
   "source": [
    "## Section 5\n",
    "Loading BLIP and setting up the cross attention heatmap with Gradcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2a7b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Inference / Agent integration ===\n",
    "# Load processor + model (finetuned path)\n",
    "\n",
    "def load_blip_model(model_dir=output_dir):\n",
    "    \n",
    "    processor = BlipProcessor.from_pretrained(model_dir)\n",
    "    model = BlipForConditionalGeneration.from_pretrained(model_dir).to(device)\n",
    "    model.eval()\n",
    "    '''\n",
    "    model_path = \"blip-finetuned-fossils\"\n",
    "    processor = BlipProcessor.from_pretrained(model_path)\n",
    "    model = BlipForConditionalGeneration.from_pretrained(model_path)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    '''\n",
    "    return processor, model\n",
    "# === Grad-CAM for BLIP cross-attention (fixed) ===\n",
    "class BlipCrossAttnGradCAMExplain:\n",
    "    \"\"\"\n",
    "    Generates per-token Grad-CAM heatmaps and textual explanations.\n",
    "    \"\"\"\n",
    "    def __init__(self, blip_model, processor, device=None):\n",
    "        self.device = device if device is not None else next(blip_model.parameters()).device\n",
    "        self.model = blip_model.to(self.device)\n",
    "        self.processor = processor\n",
    "        self.attn_maps = []\n",
    "        self.hooks = []\n",
    "        self._register_hooks()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        self.attn_maps = []\n",
    "        self.hooks = []\n",
    "        try:\n",
    "            decoder = self.model.text_decoder if hasattr(self.model, 'text_decoder') else self.model.decoder\n",
    "            layers = decoder.bert.encoder.layer\n",
    "\n",
    "            for L in layers:\n",
    "                ca = getattr(L, 'crossattention', None)\n",
    "                if ca is None:\n",
    "                    continue\n",
    "\n",
    "                def make_hook():\n",
    "                    def hook(module, inp, out):\n",
    "                        # out = (output, attn_weights)\n",
    "                        if isinstance(out, tuple) and len(out) >= 2:\n",
    "                            self.attn_maps.append(out[1])\n",
    "                    return hook\n",
    "\n",
    "                h = ca.register_forward_hook(make_hook())\n",
    "                self.hooks.append(h)\n",
    "        except Exception as e:\n",
    "            print(\"Hook registration failed:\", e)\n",
    "\n",
    "    def generate_per_token(self, pil_image, caption):\n",
    "        \"\"\"\n",
    "        Returns a list of heatmaps, one per caption token.\n",
    "        \"\"\"\n",
    "        self.attn_maps = []\n",
    "\n",
    "        # Encode image and text\n",
    "        inputs = self.processor(pil_image, text=caption, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _ = self.model(**inputs)\n",
    "\n",
    "        if len(self.attn_maps) == 0:\n",
    "            raise RuntimeError(\"No attention maps recorded.\")\n",
    "\n",
    "        # Take the last layer\n",
    "        attn = self.attn_maps[-1]  # [1, heads, seq_len_text, seq_len_image]\n",
    "        attn = attn.mean(dim=1)    # average over heads -> [1, seq_len_text, seq_len_image]\n",
    "        attn = attn[0]             # remove batch dim -> [seq_len_text, seq_len_image]\n",
    "\n",
    "        # Remove CLS token from image patches\n",
    "        attn = attn[:, 1:]  # [seq_len_text, num_patches]\n",
    "\n",
    "        # Convert each token's attention to 2D heatmap\n",
    "        num_patches = attn.shape[1]\n",
    "        side = int(np.sqrt(num_patches))\n",
    "        if side * side != num_patches:\n",
    "            raise ValueError(f\"Patch count {num_patches} is not a perfect square.\")\n",
    "\n",
    "        heatmaps = []\n",
    "        for token_attn in attn:\n",
    "            token_attn_np = token_attn.detach().cpu().numpy()\n",
    "            token_attn_np = token_attn_np - token_attn_np.min()\n",
    "            token_attn_np = token_attn_np / (token_attn_np.max() + 1e-8)\n",
    "            token_heatmap = token_attn_np.reshape(side, side)\n",
    "            token_heatmap = token_heatmap ** 0.3  # enhance contrast\n",
    "            heatmaps.append(token_heatmap)\n",
    "            \n",
    "        decoded_caption = self.processor.tokenizer.decode(\n",
    "            inputs[\"input_ids\"][0],\n",
    "            skip_special_tokens=True\n",
    "                )\n",
    "        # Split into words\n",
    "        tokens = decoded_caption.split()\n",
    "\n",
    "        return heatmaps, tokens\n",
    "        #return heatmaps, self.processor.tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1ff19f",
   "metadata": {},
   "source": [
    "## Section 6\n",
    "Attention per Token explanation and caption generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6aaef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def explain_attention_per_token(heatmaps, tokens, top_k=3):\n",
    "    \"\"\"\n",
    "    Returns human-readable explanations per token.\n",
    "    top_k: number of image patches to mention per token\n",
    "    \"\"\"\n",
    "    explanations = []\n",
    "    for token, heatmap in zip(tokens, heatmaps):\n",
    "        # Flatten heatmap and get top patch indices\n",
    "        flat_idx = heatmap.flatten().argsort()[::-1][:top_k]\n",
    "        # Convert to 2D patch coordinates\n",
    "        side = heatmap.shape[0]\n",
    "        coords = [(i // side, i % side) for i in flat_idx]\n",
    "        # Simple textual description\n",
    "        region_desc = \", \".join([f\"patch {c}\" for c in coords])\n",
    "        explanations.append(f\"Token '{token}' focuses on {region_desc}\")\n",
    "    return explanations\n",
    "\n",
    "def generate_natural_explanation(heatmaps, tokens, pil_image, top_k=3):\n",
    "    \"\"\"\n",
    "    Converts token-level heatmaps into a natural-language explanation.\n",
    "    \n",
    "    Returns a string explanation for the full caption.\n",
    "    \"\"\"\n",
    "    side = heatmaps[0].shape[0]\n",
    "    img_w, img_h = pil_image.size\n",
    "\n",
    "    token_explanations = []\n",
    "    for token, heatmap in zip(tokens, heatmaps):\n",
    "        # Flatten heatmap and get top patch indices\n",
    "        flat_idx = heatmap.flatten().argsort()[::-1][:top_k]\n",
    "        coords = [(i // side, i % side) for i in flat_idx]\n",
    "\n",
    "        # Convert to approximate image coordinates in pixels\n",
    "        regions = []\n",
    "        patch_w, patch_h = img_w / side, img_h / side\n",
    "        for r, c in coords:\n",
    "            x1, y1 = int(c * patch_w), int(r * patch_h)\n",
    "            x2, y2 = int((c + 1) * patch_w), int((r + 1) * patch_h)\n",
    "            regions.append(f\"area ({x1},{y1})-({x2},{y2})\")\n",
    "        \n",
    "        token_explanations.append(f\"'{token}' focuses on {', '.join(regions)}\")\n",
    "\n",
    "    # Combine into a paragraph\n",
    "    explanation_text = f\"The AI captioned the image as: '{' '.join(tokens)}'\\n\"\n",
    "    explanation_text += \"Attention per word:\\n\"\n",
    "    explanation_text += \"\\n\".join(token_explanations)\n",
    "    return explanation_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce88b77",
   "metadata": {},
   "source": [
    "## Section 7 \n",
    "Visualisation through GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60d6b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Visualization/Heatmap ===\n",
    "def show_gradcam_pil(pil_image, heatmap):\n",
    "    img = np.array(pil_image)\n",
    "    hmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    hmap = np.clip(hmap, 0, 1)\n",
    "\n",
    "    hmap_color = cv2.applyColorMap((hmap * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "    hmap_color = cv2.cvtColor(hmap_color, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    overlay = cv2.addWeighted(img, 0.6, hmap_color, 0.4, 0)\n",
    "    return Image.fromarray(overlay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b692332",
   "metadata": {},
   "source": [
    "## Section 8\n",
    "Running the Framework.\n",
    "This is just a simple main function that binds all segments together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ff8689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Example: load fine-tuned model (or base) and create agents ===\n",
    "if __name__ == '__main__':\n",
    "    fine_tuned_dir = \"blip-finetuned-fossils\"\n",
    "    proc, bm = load_blip_model(fine_tuned_dir)\n",
    "    bm.eval()\n",
    "\n",
    "    caption_agent = make_caption_agent(proc, bm)\n",
    "    gradcam_exp = BlipCrossAttnGradCAMExplain(bm, proc, device=device)\n",
    "\n",
    "    pil = Image.open(\"dataset/images/0001.jpg\").convert('RGB')\n",
    "    msg = {\"image\": pil}\n",
    "    msg = caption_agent(msg)\n",
    "    caption = msg['caption'] \n",
    "    clip_cap, clip_conf = clip_caption_confidence(pil, caption)\n",
    "\n",
    "    print(f\"BLIP Caption: {caption}\")\n",
    "    print(f\"CLIP Confidence: {clip_conf:.4f}\")\n",
    "\n",
    "    # Generate per-token heatmaps\n",
    "    heatmaps, tokens = gradcam_exp.generate_per_token(pil, caption)\n",
    "\n",
    "    # Generate natural-language explanation\n",
    "    explanation = generate_natural_explanation(heatmaps, tokens, pil)\n",
    "    print(\"\\nExplanation:\\n\", explanation)\n",
    "    overlay = show_gradcam_pil(pil, hm)\n",
    "    display(overlay)\n",
    "    '''\n",
    "    # Optional: visualize token-level heatmaps\n",
    "    for token, hm in zip(tokens, heatmaps):\n",
    "        overlay = show_gradcam_pil(pil, hm)\n",
    "        display(overlay)\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f594021c",
   "metadata": {},
   "source": [
    "## Section 9 \n",
    "This is the training script that was used to fine tune the new BLIP Model I chose due to some compability issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83128551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 5401\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6755' max='6755' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6755/6755 19:10, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>6.916000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.967100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.392300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.987200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.823000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>3.525900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>3.209600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>3.036500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>3.117600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.828200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.775600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.796900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.733900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.829700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.565000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.657800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2.714900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.682200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.602200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.724600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>2.618900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>2.515100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>2.638400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>2.485800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.601600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>2.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>2.655200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>2.614000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>2.558200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.479600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>2.353600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>2.221600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>2.270500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>2.487300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.251000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>2.272000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>2.509500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>2.375700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>2.368800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.367000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>2.494000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>2.239700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>2.226000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>2.279600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.242500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>2.330800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>2.325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>2.263700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>2.244600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.286900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>2.102700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>2.503500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>2.159100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>2.238900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.444700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>2.395000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>2.091500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>2.128100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>2.292700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.144800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>2.270800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>2.307900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>2.182900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>2.196400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.195400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>2.347400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>2.140700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>2.201700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>2.039800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.944900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>1.797200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>1.847100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>2.010200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>2.043900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.016300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>1.910800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>1.898300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>1.920700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>1.900100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.763500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>2.067800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>1.702100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>1.950600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>1.932200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.881900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>1.939500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>1.809700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>1.967100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>1.856800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.878200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>2.037100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>1.895200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>2.085600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>1.831400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.081300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>2.066400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>1.798700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>2.055600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>1.937400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.843900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>1.841100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>1.898400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>1.814100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>1.831600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.866600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>1.937800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>1.812900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>1.763100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>1.615300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.900500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>1.903600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>1.799500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>1.793700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>1.923100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.761900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>2.009400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>1.936600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>2.048400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2380</td>\n",
       "      <td>1.872700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.838200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2420</td>\n",
       "      <td>1.842500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>1.842100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2460</td>\n",
       "      <td>1.995100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>1.952900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.973500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>1.728900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2540</td>\n",
       "      <td>2.072900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>1.895300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2580</td>\n",
       "      <td>1.696200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.855100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2620</td>\n",
       "      <td>1.801900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>1.567700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2660</td>\n",
       "      <td>1.799300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2680</td>\n",
       "      <td>1.762600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.806400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2720</td>\n",
       "      <td>1.671000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2740</td>\n",
       "      <td>1.470900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>1.651400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2780</td>\n",
       "      <td>1.475900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.532400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2820</td>\n",
       "      <td>1.734900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2840</td>\n",
       "      <td>1.633200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2860</td>\n",
       "      <td>1.529300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>1.635600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.741000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2920</td>\n",
       "      <td>1.628900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2940</td>\n",
       "      <td>1.534000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2960</td>\n",
       "      <td>1.584000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2980</td>\n",
       "      <td>1.736800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.434500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3020</td>\n",
       "      <td>1.630900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3040</td>\n",
       "      <td>1.475800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3060</td>\n",
       "      <td>1.602500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3080</td>\n",
       "      <td>1.484700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.522900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>1.586500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3140</td>\n",
       "      <td>1.605100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3160</td>\n",
       "      <td>1.489500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3180</td>\n",
       "      <td>1.624200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.552800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3220</td>\n",
       "      <td>1.466700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>1.469700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3260</td>\n",
       "      <td>1.835600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3280</td>\n",
       "      <td>1.472400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.533400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3320</td>\n",
       "      <td>1.515400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3340</td>\n",
       "      <td>1.535900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3360</td>\n",
       "      <td>1.673800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3380</td>\n",
       "      <td>1.650700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.724300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3420</td>\n",
       "      <td>1.602400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3440</td>\n",
       "      <td>1.515200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3460</td>\n",
       "      <td>1.681000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3480</td>\n",
       "      <td>1.509200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.465100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3520</td>\n",
       "      <td>1.568000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3540</td>\n",
       "      <td>1.689400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3560</td>\n",
       "      <td>1.419500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3580</td>\n",
       "      <td>1.471900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.653500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3620</td>\n",
       "      <td>1.615200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3640</td>\n",
       "      <td>1.601800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3660</td>\n",
       "      <td>1.479700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3680</td>\n",
       "      <td>1.521800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.471600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3720</td>\n",
       "      <td>1.408500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3740</td>\n",
       "      <td>1.674200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3760</td>\n",
       "      <td>1.624600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3780</td>\n",
       "      <td>1.486900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3820</td>\n",
       "      <td>1.417000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3840</td>\n",
       "      <td>1.619400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3860</td>\n",
       "      <td>1.848300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3880</td>\n",
       "      <td>1.465800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.550800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3920</td>\n",
       "      <td>1.604200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3940</td>\n",
       "      <td>1.436400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3960</td>\n",
       "      <td>1.619100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3980</td>\n",
       "      <td>1.676400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.453300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4020</td>\n",
       "      <td>1.343800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4040</td>\n",
       "      <td>1.521500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4060</td>\n",
       "      <td>1.388100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4080</td>\n",
       "      <td>1.221400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.260100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4120</td>\n",
       "      <td>1.470400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4140</td>\n",
       "      <td>1.416000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4160</td>\n",
       "      <td>1.265200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4180</td>\n",
       "      <td>1.095000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.288600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4220</td>\n",
       "      <td>1.296400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4240</td>\n",
       "      <td>1.315000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4260</td>\n",
       "      <td>1.306700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4280</td>\n",
       "      <td>1.269200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>1.488800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4320</td>\n",
       "      <td>1.192900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4340</td>\n",
       "      <td>1.044500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4360</td>\n",
       "      <td>1.308800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4380</td>\n",
       "      <td>1.361500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.259400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4420</td>\n",
       "      <td>1.264300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4440</td>\n",
       "      <td>1.220300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4460</td>\n",
       "      <td>1.286200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4480</td>\n",
       "      <td>1.230600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.160100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4520</td>\n",
       "      <td>1.306200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4540</td>\n",
       "      <td>1.338500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4560</td>\n",
       "      <td>1.248400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4580</td>\n",
       "      <td>1.382900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.223000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4620</td>\n",
       "      <td>1.160500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4640</td>\n",
       "      <td>1.329400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4660</td>\n",
       "      <td>1.416000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4680</td>\n",
       "      <td>1.210800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>1.243000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4720</td>\n",
       "      <td>1.198200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4740</td>\n",
       "      <td>1.348400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4760</td>\n",
       "      <td>1.241200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4780</td>\n",
       "      <td>1.291200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.146100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4820</td>\n",
       "      <td>1.206200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4840</td>\n",
       "      <td>1.304900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4860</td>\n",
       "      <td>1.317300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4880</td>\n",
       "      <td>1.225400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>1.176200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4920</td>\n",
       "      <td>1.297700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4940</td>\n",
       "      <td>1.225500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4960</td>\n",
       "      <td>1.412200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4980</td>\n",
       "      <td>1.110100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.239500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5020</td>\n",
       "      <td>1.178000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5040</td>\n",
       "      <td>1.274300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5060</td>\n",
       "      <td>1.327500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5080</td>\n",
       "      <td>1.253700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>1.456600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5120</td>\n",
       "      <td>1.207900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5140</td>\n",
       "      <td>1.340700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5160</td>\n",
       "      <td>1.210300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5180</td>\n",
       "      <td>1.127400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.342400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5220</td>\n",
       "      <td>1.298700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5240</td>\n",
       "      <td>1.230800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5260</td>\n",
       "      <td>1.024700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5280</td>\n",
       "      <td>1.273300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>1.178700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5320</td>\n",
       "      <td>1.249600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5340</td>\n",
       "      <td>1.286200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5360</td>\n",
       "      <td>1.332900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5380</td>\n",
       "      <td>1.306800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.320100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5420</td>\n",
       "      <td>0.953700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5440</td>\n",
       "      <td>1.062200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5460</td>\n",
       "      <td>0.865000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5480</td>\n",
       "      <td>1.080600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.085300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5520</td>\n",
       "      <td>0.968100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5540</td>\n",
       "      <td>0.917400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5560</td>\n",
       "      <td>1.071500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5580</td>\n",
       "      <td>1.043000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5620</td>\n",
       "      <td>1.046100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5640</td>\n",
       "      <td>1.106200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5660</td>\n",
       "      <td>1.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5680</td>\n",
       "      <td>0.994400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>1.031900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5720</td>\n",
       "      <td>0.929800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5740</td>\n",
       "      <td>1.062700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5760</td>\n",
       "      <td>0.947000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5780</td>\n",
       "      <td>0.997100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>1.167800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5820</td>\n",
       "      <td>0.962700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5840</td>\n",
       "      <td>1.057500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5860</td>\n",
       "      <td>1.139900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5880</td>\n",
       "      <td>1.009200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.994800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5920</td>\n",
       "      <td>0.907300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5940</td>\n",
       "      <td>1.034600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5960</td>\n",
       "      <td>0.857300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5980</td>\n",
       "      <td>1.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.088500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6020</td>\n",
       "      <td>1.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6040</td>\n",
       "      <td>0.970900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6060</td>\n",
       "      <td>0.878900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6080</td>\n",
       "      <td>0.979900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>1.058800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6120</td>\n",
       "      <td>1.020700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6140</td>\n",
       "      <td>0.961000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6160</td>\n",
       "      <td>0.984000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6180</td>\n",
       "      <td>1.129300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.164800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6220</td>\n",
       "      <td>1.076100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6240</td>\n",
       "      <td>0.930200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6260</td>\n",
       "      <td>0.924700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6280</td>\n",
       "      <td>1.051300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>1.028700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6320</td>\n",
       "      <td>1.053900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6340</td>\n",
       "      <td>0.971500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6360</td>\n",
       "      <td>1.115600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6380</td>\n",
       "      <td>1.026600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.961300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6420</td>\n",
       "      <td>1.143300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6440</td>\n",
       "      <td>0.935300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6460</td>\n",
       "      <td>0.885500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6480</td>\n",
       "      <td>1.056400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.168400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6520</td>\n",
       "      <td>0.931300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6540</td>\n",
       "      <td>0.998700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6560</td>\n",
       "      <td>0.957100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6580</td>\n",
       "      <td>1.051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.985100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6620</td>\n",
       "      <td>0.915800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6640</td>\n",
       "      <td>1.024600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6660</td>\n",
       "      <td>1.069400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6680</td>\n",
       "      <td>1.065600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.895600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6720</td>\n",
       "      <td>0.981200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6740</td>\n",
       "      <td>0.984600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, Trainer, TrainingArguments\n",
    "\n",
    "# ----------------------\n",
    "# Dataset\n",
    "# ----------------------\n",
    "class FossilCaptionDataset(Dataset):\n",
    "    def __init__(self, json_path, image_folder, processor):\n",
    "        self.data = json.load(open(json_path))\n",
    "        self.keys = list(self.data.keys())\n",
    "        self.image_folder = image_folder\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.keys[idx]\n",
    "        caption = self.data[img_name]\n",
    "\n",
    "        img_path = os.path.join(self.image_folder, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        inputs = self.processor(\n",
    "            images=image,\n",
    "            text=caption,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": inputs[\"pixel_values\"].squeeze(0),\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "\n",
    "# ----------------------\n",
    "# Paths and model\n",
    "# ----------------------\n",
    "json_path = \"dataset/dataset.json\"\n",
    "image_folder = \"dataset/images/\"\n",
    "\n",
    "model_name = \"Salesforce/blip-image-captioning-base\"\n",
    "processor = BlipProcessor.from_pretrained(model_name)\n",
    "model = BlipForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "dataset = FossilCaptionDataset(json_path, image_folder, processor)\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "\n",
    "# ----------------------\n",
    "# Training arguments\n",
    "# ----------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"blip-finetuned-fossils\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=20,\n",
    "    save_steps=500,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# ----------------------\n",
    "# Collate function for BLIP\n",
    "# ----------------------\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([x[\"pixel_values\"] for x in batch])\n",
    "    input_ids = torch.stack([x[\"input_ids\"] for x in batch])\n",
    "    attention_mask = torch.stack([x[\"attention_mask\"] for x in batch])\n",
    "\n",
    "    # Mask padding tokens in labels\n",
    "    labels = input_ids.clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "# ----------------------\n",
    "# Custom Trainer to avoid num_items_in_batch error\n",
    "# ----------------------\n",
    "class BLIPTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(\n",
    "            pixel_values=inputs[\"pixel_values\"],\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# ----------------------\n",
    "# Trainer setup\n",
    "# ----------------------\n",
    "trainer = BLIPTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "# ----------------------\n",
    "# Train\n",
    "# ----------------------\n",
    "trainer.train()\n",
    "\n",
    "# Save model and processor\n",
    "model.save_pretrained(\"blip-finetuned-fossils\")\n",
    "processor.save_pretrained(\"blip-finetuned-fossils\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fossil_env_311)",
   "language": "python",
   "name": "fossil_env_311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
