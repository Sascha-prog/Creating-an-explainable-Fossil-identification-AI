{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8444c83a",
   "metadata": {},
   "source": [
    "Getting the dataset together\n",
    "After doing some data cleaning and validation, the records should be in the same format and complete. As much as was possible with this kind of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ea7b0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: open_clip_torch\n",
      "Version: 3.2.0\n",
      "Summary: Open reproduction of consastive language-image pretraining (CLIP) and related.\n",
      "Home-page: https://github.com/mlfoundations/open_clip\n",
      "Author: Gabriel Ilharco, Mitchell Wortsman, Romain Beaumont\n",
      "Author-email: Ross Wightman <ross@huggingface.co>\n",
      "License: MIT\n",
      "Location: c:\\Users\\Sascha\\Desktop\\LegaSea Model\\fossil_env_311\\Lib\\site-packages\n",
      "Requires: ftfy, huggingface-hub, regex, safetensors, timm, torch, torchvision, tqdm\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!python -m pip show open_clip_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f57b6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sascha\\AppData\\Local\\Temp\\ipykernel_29448\\3098096239.py:26: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(strip_tags)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è No existing mapping found. Starting fresh.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 2472/5402 [14:34<17:12,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping invalid URL: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5402/5402 [31:07<00:00,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Done! Downloaded 5401 images.\n",
      "üìÅ Images saved in: Dataset\\images\n",
      "üìÑ Mapping saved in: Dataset\\images_mapping.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "EXCEL_FILE = \"Dataset\\SelectiveData.xlsx\"              # Input Excel file\n",
    "IMAGE_URL_COLUMN = \"images0\"        # Column with image URLs\n",
    "METADATA_COLUMNS = [\"reviewer_notes\", \"category\", \"type_code\", \"ShortOrLong\"]  # Other columns you want in mapping\n",
    "OUTPUT_FOLDER = \"Dataset\\images\"   # Folder to save images\n",
    "OUTPUT_MAPPING_FILE = \"Dataset\\images_mapping.csv\"  # Output mapping file\n",
    "# =====================\n",
    "# Define a simple function to remove HTML tags\n",
    "def strip_tags(text):\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r\"<[^>]*>\", \"\", text)\n",
    "    return text\n",
    "\n",
    "# Create output folder\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# Read Excel\n",
    "df = pd.read_excel(EXCEL_FILE, dtype=str)\n",
    "df = df.applymap(strip_tags)\n",
    "\n",
    "# i wanna resume after pausing\n",
    "# Load existing mapping file if it exists\n",
    "if os.path.exists(OUTPUT_MAPPING_FILE):\n",
    "    existing_mapping = pd.read_csv(OUTPUT_MAPPING_FILE)\n",
    "    if \"url\" in existing_mapping.columns:\n",
    "        downloaded_urls = set(existing_mapping[\"url\"].dropna().tolist())\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è 'url' column missing in existing mapping file. Continuing without skip logic.\")\n",
    "        downloaded_urls = set()\n",
    "    counter = len(existing_mapping) + 1  # Continue numbering\n",
    "    print(f\"‚ÑπÔ∏è Found existing mapping with {len(existing_mapping)} images. Resuming at {counter:04d}.\")\n",
    "else:\n",
    "    existing_mapping = pd.DataFrame()\n",
    "    downloaded_urls = set()\n",
    "    counter = 1\n",
    "    print(\"‚ÑπÔ∏è No existing mapping found. Starting fresh.\")\n",
    "\n",
    "records = []\n",
    "\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    url = row[IMAGE_URL_COLUMN]\n",
    "\n",
    "# Skip missing or invalid URLs\n",
    "    if pd.isna(url) or not str(url).startswith(\"http\"):\n",
    "        print(f\"‚ö†Ô∏è Skipping invalid URL: {url}\")\n",
    "        continue\n",
    "    \n",
    "    if url in downloaded_urls:\n",
    "        continue\n",
    "    \n",
    "    url = str(url)\n",
    "    image_id = f\"{counter:04d}\"\n",
    "    \n",
    "    ext = os.path.splitext(url.split(\"?\")[0])[1].lower()\n",
    "    if ext not in [\".jpg\", \".jpeg\", \".png\", \".gif\", \".webp\"]:\n",
    "        ext = \".jpg\"\n",
    "    \n",
    "    filename = f\"{image_id}{ext}\"\n",
    "    filepath = os.path.join(OUTPUT_FOLDER, filename)\n",
    "\n",
    "    try:\n",
    "        # Download image\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        # Create mapping record\n",
    "        record = {\n",
    "            \"id\": image_id,\n",
    "            \"filename\": filename,\n",
    "            \"url\": url\n",
    "        }\n",
    "\n",
    "        # Add metadata columns dynamically\n",
    "        for col in METADATA_COLUMNS:\n",
    "            record[col] = row.get(col, None)\n",
    "\n",
    "        records.append(record)\n",
    "        counter += 1\n",
    "       \n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to download {url}: {e}\")\n",
    "\n",
    "# Save mapping file\n",
    "# Save mapping file\n",
    "#mapping_df = pd.DataFrame(records)\n",
    "#mapping_df.to_csv(OUTPUT_MAPPING_FILE, index=False)\n",
    "if not existing_mapping.empty:\n",
    "    mapping_df = pd.concat([existing_mapping, pd.DataFrame(records)], ignore_index=True)\n",
    "else:\n",
    "    mapping_df = pd.DataFrame(records)\n",
    "\n",
    "mapping_df.to_csv(\n",
    "    OUTPUT_MAPPING_FILE,\n",
    "    index=False,\n",
    "    quoting=csv.QUOTE_ALL,\n",
    "    escapechar='\\\\',\n",
    "    encoding='utf-8-sig'\n",
    ")\n",
    "print(f\"\\n‚úÖ Done! Downloaded {len(records)} images.\")\n",
    "print(f\"üìÅ Images saved in: {OUTPUT_FOLDER}\")\n",
    "print(f\"üìÑ Mapping saved in: {OUTPUT_MAPPING_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feec780b",
   "metadata": {},
   "source": [
    "Imports and Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ba964be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel, Blip2Processor, Blip2ForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchcam.methods import SmoothGradCAMpp\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8c3fd1",
   "metadata": {},
   "source": [
    "Loading my Dataset\n",
    "Structure:\n",
    "fossil_dataset/\n",
    "  train/\n",
    "    ammonite/\n",
    "    trilobite/\n",
    "    coral/\n",
    "  val/\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e88adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=\"fossil_dataset\")\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3eb5a0",
   "metadata": {},
   "source": [
    "Load and fine tune CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581ac228",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\"\n",
    "model = CLIPModel.from_pretrained(model_id)\n",
    "processor = CLIPProcessor.from_pretrained(model_id)\n",
    "\n",
    "def preprocess(examples):\n",
    "    return processor(\n",
    "        text=examples[\"label\"],\n",
    "        images=examples[\"image\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "proc_dataset = dataset.map(preprocess, batched=True)\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./clip_fossil_finetuned\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=5e-6,\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    save_steps=500\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=proc_dataset[\"train\"],\n",
    "    eval_dataset=proc_dataset[\"test\"]\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa6e003",
   "metadata": {},
   "source": [
    "Add the prediction fucntion\n",
    "Usage:\n",
    "labels = [\"ammonite\", \"trilobite\", \"coral\", \"crinoid\"]\n",
    "prediction, confidence = classify_image(\"example_fossil.jpg\", labels)\n",
    "print(f\"Predicted fossil: {prediction} (Confidence: {confidence:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44963f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image(image_path, labels):\n",
    "    image = Image.open(image_path)\n",
    "    inputs = processor(text=labels, images=image, return_tensors=\"pt\", padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    probs = outputs.logits_per_image.softmax(dim=1)\n",
    "    best_idx = probs.argmax(dim=1).item()\n",
    "    return labels[best_idx], probs[0, best_idx].item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565a9bf1",
   "metadata": {},
   "source": [
    "Add BLIP2 -> fine tune for domain\n",
    "usage:\n",
    "explanation = generate_explanation(\"example_fossil.jpg\", prediction)\n",
    "print(\"Explanation:\", explanation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca71be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "blip_id = \"Salesforce/blip2-flan-t5-base\"\n",
    "blip_processor = Blip2Processor.from_pretrained(blip_id)\n",
    "blip_model = Blip2ForConditionalGeneration.from_pretrained(blip_id)\n",
    "\n",
    "def generate_explanation(image_path, fossil_label):\n",
    "    image = Image.open(image_path)\n",
    "    prompt = f\"This is likely a {fossil_label}. Explain which visual features indicate this identification.\"\n",
    "    inputs = blip_processor(images=image, text=prompt, return_tensors=\"pt\")\n",
    "    output = blip_model.generate(**inputs, max_new_tokens=100)\n",
    "    explanation = blip_processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7522299",
   "metadata": {},
   "source": [
    "GRAD CAM\n",
    "usage:\n",
    "visualize_attention(\"example_fossil.jpg\", text_prompt=prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547bf852",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "cam_extractor = SmoothGradCAMpp(model.vision_model)\n",
    "\n",
    "def visualize_attention(image_path, text_prompt=\"a fossil\"):\n",
    "    image = Image.open(image_path)\n",
    "    inputs = processor(images=image, text=[text_prompt], return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    cams = cam_extractor(inputs[\"pixel_values\"])\n",
    "    cam = cams[0][0]\n",
    "    heatmap = to_pil_image(cam / cam.max())\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(image)\n",
    "    plt.imshow(heatmap, alpha=0.5, cmap=\"jet\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b091eae",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Full pipeline\n",
    "fossil_pipeline(\"example_fossil.jpg\", [\"ammonite\", \"trilobite\", \"coral\", \"crinoid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22713e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fossil_pipeline(image_path, labels):\n",
    "    label, conf = classify_image(image_path, labels)\n",
    "    print(f\"ü¶¥ Predicted fossil: {label} ({conf:.2f})\")\n",
    "    explanation = generate_explanation(image_path, label)\n",
    "    print(\"\\nüí¨ Explanation:\", explanation)\n",
    "    visualize_attention(image_path, text_prompt=label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72ca986",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Imports and Setup\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torchcam.methods import SmoothGradCAMpp\n",
    "from torchcam.utils import overlay_mask\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ===============================\n",
    "# Step 1: Load Your Dataset\n",
    "# ===============================\n",
    "\n",
    "# Replace this path with your dataset CSV\n",
    "dataset_csv = \"fossil_dataset/annotations.csv\"\n",
    "dataset_dir = \"fossil_dataset/images/\"\n",
    "\n",
    "# Load CSV with Hugging Face datasets\n",
    "dataset = load_dataset(\"csv\", data_files=dataset_csv)\n",
    "\n",
    "# Example row\n",
    "print(dataset['train'][0])\n",
    "\n",
    "# ===============================\n",
    "# Step 2: Load Pretrained CLIP\n",
    "# ===============================\n",
    "\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "model = CLIPModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# ===============================\n",
    "# Step 3: Preprocessing Function\n",
    "# ===============================\n",
    "\n",
    "def preprocess(example):\n",
    "    image_path = os.path.join(dataset_dir, example['image'])\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(text=example['caption'], images=image, return_tensors=\"pt\", padding=True)\n",
    "    return inputs\n",
    "\n",
    "# Test preprocessing\n",
    "sample_inputs = preprocess(dataset['train'][0])\n",
    "print(sample_inputs.keys())\n",
    "\n",
    "# ===============================\n",
    "# Step 4: Fine-Tuning Setup\n",
    "# ===============================\n",
    "\n",
    "# Simple PyTorch DataLoader\n",
    "class FossilDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.dataset = hf_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataset[idx]\n",
    "        image_path = os.path.join(dataset_dir, row['image'])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        caption = row['caption']\n",
    "        return image, caption\n",
    "\n",
    "train_dataset = FossilDataset(dataset['train'])\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-6)\n",
    "\n",
    "# ===============================\n",
    "# Step 5: Fine-Tuning Loop (Simplified)\n",
    "# ===============================\n",
    "\n",
    "for epoch in range(1):  # adjust epochs\n",
    "    for images, captions in train_loader:\n",
    "        # Prepare batch\n",
    "        inputs = processor(text=list(captions), images=list(images), return_tensors=\"pt\", padding=True).to(device)\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # Simple CLIP contrastive loss\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        logits_per_text = outputs.logits_per_text\n",
    "        loss = ((logits_per_image - logits_per_text.T)**2).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} done. Loss: {loss.item():.4f}\")\n",
    "\n",
    "# ===============================\n",
    "# Step 6: Visual Explainability\n",
    "# ===============================\n",
    "\n",
    "# Initialize Grad-CAM\n",
    "cam_extractor = SmoothGradCAMpp(model.visual)\n",
    "\n",
    "def show_gradcam(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    outputs = model.get_image_features(**inputs)\n",
    "    target_class = outputs.argmax(dim=-1).item()\n",
    "    \n",
    "    activation_map = cam_extractor(outputs[0].unsqueeze(0), target_class)\n",
    "    \n",
    "    # Overlay\n",
    "    result = overlay_mask(transforms.ToTensor()(image), transforms.ToPILImage()(activation_map[0].cpu()), alpha=0.5)\n",
    "    plt.imshow(result)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example\n",
    "show_gradcam(os.path.join(dataset_dir, dataset['train'][0]['image']))\n",
    "\n",
    "# ===============================\n",
    "# Step 7: Textual Explanation\n",
    "# ===============================\n",
    "\n",
    "def explain_text(image_path, candidate_captions):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(text=candidate_captions, images=[image]*len(candidate_captions), return_tensors=\"pt\", padding=True).to(device)\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # Similarity scores\n",
    "    logits = outputs.logits_per_image\n",
    "    for caption, score in zip(candidate_captions, logits[0]):\n",
    "        print(f\"{caption}: {score.item():.4f}\")\n",
    "\n",
    "# Example\n",
    "candidate_texts = [\n",
    "    \"Trilobite fragment\",\n",
    "    \"Ammonite shell\",\n",
    "    \"Unknown fossil fragment\"\n",
    "]\n",
    "explain_text(os.path.join(dataset_dir, dataset['train'][0]['image']), candidate_texts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fossil_env_311)",
   "language": "python",
   "name": "fossil_env_311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
